{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQjHV8nwSu0E"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Class incremental learning on the GTSRB dataset\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This notebook contains several code snippets to help for your project:\n",
        "\n",
        "- data loaders\n",
        "- A baseline for incremental learning using fine-tuning\n",
        "- Examples of how to use Weight & Biases for logging your results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "c6J4ap8k7k79"
      },
      "outputs": [],
      "source": [
        "####################################\n",
        "### Useful imports\n",
        "############################\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, ConcatDataset, Subset\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision import transforms, datasets\n",
        "import torchvision.models as models\n",
        "from torchvision.transforms import v2\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import time, os\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BdBUgULZ-75F"
      },
      "outputs": [],
      "source": [
        "# # Useful if you want to store intermediate results on your drive\n",
        "# from google.colab import drive\n",
        "\n",
        "# # Useful if you want to store intermediate results on your drive from google.colab import drive\n",
        "\n",
        "# drive.mount('/content/gdrive/')\n",
        "# DATA_DIR =  '/content/gdrive/MyDrive/teaching/ENSTA/2024'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLLmoj5nhhli",
        "outputId": "ce9cdda5-6008-454c-8ca9-a697de680572"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Mar 15 19:31:28 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "  !nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdRoUYxVqicA"
      },
      "source": [
        "## Data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5yjcnoUN7xVJ"
      },
      "outputs": [],
      "source": [
        "# Define transformations\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(32),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize((0.3403, 0.3121, 0.3214), (0.2724, 0.2608, 0.2669)) # GTSRB stats\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(32),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize((0.3403, 0.3121, 0.3214), (0.2724, 0.2608, 0.2669))\n",
        "])\n",
        "\n",
        "transform_train = v2.Compose([\n",
        "    #v2.Grayscale(),\n",
        "    #v2.RandomResizedCrop(32),\n",
        "    v2.Resize((32,32)),\n",
        "    v2.ToImage(),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    #v2.Normalize((0.3403, 0.3121, 0.3214), (0.2724, 0.2608, 0.2669)) # GTSRB stats\n",
        "])\n",
        "\n",
        "transform_test = v2.Compose([\n",
        "    #v2.Grayscale(),\n",
        "    v2.Resize((32,32)),\n",
        "    v2.ToImage(),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    #v2.Normalize((0.3403, 0.3121, 0.3214), (0.2724, 0.2608, 0.2669)) # GTSRB stats\n",
        "])\n",
        "\n",
        "# Define dataset and dataloader\n",
        "def get_dataset(root_dir, transform, train=True):\n",
        "    dataset = datasets.GTSRB(root=root_dir, split='train' if train else 'test', download=True, transform=transform)\n",
        "    target = [data[1] for data in dataset]\n",
        "    return dataset, target\n",
        "\n",
        "def create_dataloader(dataset, targets, current_classes, batch_size, shuffle):\n",
        "    indices = [i for i, label in enumerate(targets) if label in current_classes]\n",
        "    subset = Subset(dataset, indices)\n",
        "    dataloader = DataLoader(subset, batch_size=batch_size, shuffle=shuffle)\n",
        "    return dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "KbdRiiin8q6n",
        "outputId": "d8ccd41c-ba9d-4068-9b1d-eb99912f1e8c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "URLError",
          "evalue": "<urlopen error [Errno 111] Connection refused>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1347\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[0m\u001b[1;32m   1349\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1298\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1057\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    995\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    997\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1468\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    961\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http.client.connect\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 962\u001b[0;31m         self.sock = self._create_connection(\n\u001b[0m\u001b[1;32m    963\u001b[0m             (self.host,self.port), self.timeout, self.source_address)\n",
            "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, all_errors)\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mExceptionGroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"create_connection failed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, all_errors)\u001b[0m\n\u001b[1;32m    847\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m             \u001b[0;31m# Break explicitly a reference cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-a327c12be1b3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mroot_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGTSRB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGTSRB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/gtsrb.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, split, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/gtsrb.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             download_and_extract_archive(\n\u001b[0m\u001b[1;32m     89\u001b[0m                 \u001b[0;34mf\"{base_url}GTSRB-Training_fixed.zip\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mdownload_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m     \u001b[0mdownload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# expand redirect chain if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_redirect_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_hops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_redirect_hops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# check if file is located on Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36m_get_redirect_url\u001b[0;34m(url, max_hops)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_hops\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'urllib.Request'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0m\u001b[1;32m    537\u001b[0m                                   '_open', req)\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1391\u001b[0;31m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0m\u001b[1;32m   1392\u001b[0m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[1;32m   1350\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1351\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1352\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno 111] Connection refused>"
          ]
        }
      ],
      "source": [
        "# Loads datasets (on your local computer)\n",
        "root_dir = '/home/stephane/Documents/Onera/Cours/ENSTA/2025/data'\n",
        "\n",
        "# Loads datasets (on Colab local computer)\n",
        "# root_dir = './data'\n",
        "\n",
        "train_dataset = datasets.GTSRB(root=root_dir, split='train', download=True, transform=transform_train)\n",
        "test_dataset = datasets.GTSRB(root=root_dir, split='test', download=True, transform=transform_test)\n",
        "\n",
        "print(f\"Train dataset contains {len(train_dataset)} images\")\n",
        "print(f\"Test dataset contains {len(test_dataset)} images\")\n",
        "\n",
        "# Loads target id lists and class names (not in torchvision dataset)\n",
        "import csv\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/stepherbin/teaching/refs/heads/master/IOGS/projet/test_target.csv', delimiter=',', header=None)\n",
        "test_target = data.to_numpy().squeeze().tolist()\n",
        "\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/stepherbin/teaching/refs/heads/master/IOGS/projet/train_target.csv', delimiter=',', header=None)\n",
        "train_target = data.to_numpy().squeeze().tolist()\n",
        "\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/stepherbin/teaching/refs/heads/master/IOGS/projet/signnames.csv')\n",
        "class_names = data['SignName'].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9om6EjQqqVD"
      },
      "source": [
        "## Display of images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIrmJhgmcL8u"
      },
      "outputs": [],
      "source": [
        "nclasses = len(np.unique(train_target))\n",
        "all_classes = list(range(nclasses))\n",
        "#random.shuffle(all_classes)\n",
        "classes_per_task = 8\n",
        "current_classes = []\n",
        "\n",
        "task = 0\n",
        "task_classes = all_classes[task * classes_per_task : (task + 1) * classes_per_task]\n",
        "current_classes.extend(task_classes)\n",
        "batch_size = 64\n",
        "\n",
        "# Create data for first task\n",
        "train_loader = create_dataloader(train_dataset, train_target, current_classes, batch_size, shuffle = True)\n",
        "test_loader = create_dataloader(train_dataset, train_target, current_classes, batch_size, shuffle = True)\n",
        "\n",
        "# Displays a few examples\n",
        "def show(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
        "\n",
        "sample,targets = next(iter(train_loader))\n",
        "show(make_grid(sample))\n",
        "plt.show()\n",
        "\n",
        "print(sample.shape)     ## 64 is the batch\n",
        "                        ## 1 for grey values --  3 for RGB\n",
        "                        ## 32x32 for mage size (small here)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrkguKhbC4uo"
      },
      "outputs": [],
      "source": [
        "test_loader = create_dataloader(train_dataset, train_target, all_classes, batch_size, shuffle = True)\n",
        "\n",
        "# Get the data from the test set and computes statistics\n",
        "# gtsrbtest_gt = []\n",
        "# for _, targets in test_loader:\n",
        "#   gtsrbtest_gt += targets.numpy().tolist()\n",
        "# print(len(gtsrbtest_gt))\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "label_counts = Counter(test_target).most_common()\n",
        "for l, c in label_counts:\n",
        "    print(c, '\\t', l, '\\t', class_names[l])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPAkBgRiqyyf"
      },
      "source": [
        "## Simple networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0z2ycNRS-xnq"
      },
      "outputs": [],
      "source": [
        "# Define a simple CNN model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self,n_out=10, n_in=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Put the layers here\n",
        "        self.conv1 = nn.Conv2d(n_in, 32, kernel_size=5, padding=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "\n",
        "        self.fc = nn.Linear(4096, n_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.conv1(x)) ## l'image 1x32x32 devient 32x32x32\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2) ## puis 32x16x16\n",
        "        x = F.leaky_relu(self.conv2(x)) ## puis devient 64x16x16\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2) ## puis devient 64x8x8\n",
        "        x = F.leaky_relu(self.conv3(x)) ## pas de changement\n",
        "\n",
        "        x = x.view(-1,4096) ## 64x8x8 devient 4096\n",
        "\n",
        "        x = self.fc(x) ## on finit exactement de la même façon\n",
        "\n",
        "        return x\n",
        "\n",
        "# Another simple model (compare them using torchinfo below)\n",
        "class SimpleCNN2(nn.Module):\n",
        "    def __init__(self, n_out=10, n_in=1):\n",
        "        super(SimpleCNN2, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(n_in, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
        "        self.fc = nn.Linear(128, n_out)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 8 * 8)  # Flatten the tensor\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gIfiLiF-vhv"
      },
      "outputs": [],
      "source": [
        "!pip install torchinfo\n",
        "from torchinfo import summary\n",
        "\n",
        "model = SimpleCNN2(n_out=10, n_in=3)\n",
        "model.to(device)\n",
        "print(summary(model, input_size=(batch_size, 3, 32, 32)))\n",
        "\n",
        "model = SimpleCNN(n_out=10, n_in=3)\n",
        "model.to(device)\n",
        "print(summary(model, input_size=(batch_size, 3, 32, 32)))\n",
        "\n",
        "#print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f04OhO2GkqzO"
      },
      "source": [
        "## Baseline for incremental learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuVL2YbW4T_a"
      },
      "outputs": [],
      "source": [
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.init as init\n",
        "\n",
        "# Evaluation\n",
        "def evaluate(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, ncols=80):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "# Simple Training loop\n",
        "def train(model, train_loader, optimizer, criterion, device, epoch):\n",
        "    model.train()\n",
        "\n",
        "    for images, labels in tqdm(train_loader, ncols=80,  desc=\"Epoch {}\".format(epoch)):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def initialize_weights(module):\n",
        "    \"\"\"Initializes the weights of a PyTorch module using Xavier/Glorot initialization.\"\"\"\n",
        "    if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):  # Check for relevant layers\n",
        "        init.xavier_uniform_(module.weight) #Xavier uniform initialization\n",
        "        if module.bias is not None:\n",
        "            init.zeros_(module.bias)  # Initialize bias to zero\n",
        "    elif isinstance(module, (nn.BatchNorm2d, nn.LayerNorm, nn.GroupNorm)): #Initialize normalization layers\n",
        "        if module.weight is not None:\n",
        "            init.ones_(module.weight)\n",
        "        if module.bias is not None:\n",
        "            init.zeros_(module.bias)\n",
        "\n",
        "\n",
        "# Main training loop for incremental learning\n",
        "def incremental_learning(model, train_dataset, train_target, test_dataset, test_target,\n",
        "                         num_tasks, classes_per_task, batch_size, num_epochs, lr, device):\n",
        "    nclasses = len(np.unique(train_target))\n",
        "    all_classes = list(range(nclasses))\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    current_classes = []\n",
        "    accuracies = []\n",
        "\n",
        "    for task in range(num_tasks):\n",
        "        task_classes = all_classes[task * classes_per_task : (task + 1) * classes_per_task]\n",
        "        current_classes.extend(task_classes)\n",
        "\n",
        "        train_loader = create_dataloader(train_dataset, train_target, task_classes, batch_size, shuffle = True)\n",
        "        test_loader = create_dataloader(test_dataset, test_target, current_classes, batch_size, shuffle = False)\n",
        "\n",
        "        if task == 0:\n",
        "            model.fc = nn.Linear(model.fc.in_features, len(current_classes)).to(device)\n",
        "        else:\n",
        "            # Expand the output layer for new classes\n",
        "            old_weight = model.fc.weight.data\n",
        "            old_bias = model.fc.bias.data\n",
        "            model.fc = nn.Linear(model.fc.in_features, len(current_classes)).to(device)\n",
        "            model.fc.weight.data[:len(old_weight)] = old_weight\n",
        "            model.fc.bias.data[:len(old_bias)] = old_bias\n",
        "\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "        scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "\n",
        "        print(f\"Starting Task {task+1} - Training on classes: {task_classes}\")\n",
        "        for epoch in range(num_epochs): # Adjust number of epochs as needed\n",
        "            train(model, train_loader, optimizer, criterion, device, epoch)\n",
        "            scheduler.step()\n",
        "            accuracy = evaluate(model, train_loader, device)\n",
        "            print(f\"Task {task+1}, Epoch {epoch+1}: Accuracy Train = {accuracy:.2f}%\")\n",
        "        accuracy = evaluate(model, test_loader, device)\n",
        "        accuracies.append(accuracy)\n",
        "        print(f\"Task {task+1}: Accuracy Test = {accuracy:.2f}%\")\n",
        "\n",
        "    return accuracies\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uio21fDojgTD"
      },
      "source": [
        "## Weight & Biases\n",
        "\n",
        "You can use this environement to log your learning.\n",
        "\n",
        "The code below provides a version of the class incremental function that stores learning curves and the seauence of accuracies for each increment of classes.\n",
        "\n",
        "Tu use it, create an account at: https://wandb.ai/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B98gHkBTHUbX"
      },
      "outputs": [],
      "source": [
        "###################################\n",
        "##### For using Weight & Biases\n",
        "###############\n",
        "\n",
        "!pip install wandb -qU\n",
        "\n",
        "import wandb\n",
        "\n",
        "wandb.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AC_ixhkpOFZj"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "# Simple Training loop\n",
        "def train_wandb(model, train_loader, optimizer, criterion, device, epoch):\n",
        "\n",
        "    step_ct = 0\n",
        "    n_steps_per_epoch = math.ceil(len(train_loader.dataset) / train_loader.batch_size)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for step, (images, labels) in tqdm(enumerate(train_loader), ncols=80,  desc=\"Epoch {}\".format(epoch)):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        metrics = {\"train/train_loss\": loss}\n",
        "        # metrics = {\"train/train_loss\": loss,\n",
        "        #             \"train/epoch\": (step + 1 + (n_steps_per_epoch * epoch)) / n_steps_per_epoch}\n",
        "\n",
        "        if step + 1 < n_steps_per_epoch:\n",
        "          # Log train metrics to wandb\n",
        "          wandb.log(metrics)\n",
        "        step_ct += 1\n",
        "\n",
        "\n",
        "# Main training loop for incremental learning\n",
        "def incremental_learning_wandb(model, train_dataset, train_target, test_dataset, test_target,\n",
        "                         num_tasks, classes_per_task, batch_size, num_epochs, lr, device, non_incremental = False):\n",
        "    nclasses = len(np.unique(train_target))\n",
        "    all_classes = list(range(nclasses))\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    current_classes = []\n",
        "    accuracies = []\n",
        "\n",
        "    # Copy your config\n",
        "    config = wandb.config\n",
        "\n",
        "    wandb.define_metric(\"task\")\n",
        "    wandb.define_metric(\"incremental_accuracy\", step_metric=\"task\")\n",
        "\n",
        "    for task in range(num_tasks):\n",
        "        if non_incremental == True: # Learn from all available data\n",
        "          task_classes = all_classes[0 : (task + 1) * classes_per_task]\n",
        "          current_classes = task_classes\n",
        "          model.apply(initialize_weights)\n",
        "        else:\n",
        "          task_classes = all_classes[task * classes_per_task : (task + 1) * classes_per_task]\n",
        "          current_classes.extend(task_classes)\n",
        "\n",
        "        train_loader = create_dataloader(train_dataset, train_target, task_classes, batch_size, shuffle = True)\n",
        "        test_loader = create_dataloader(test_dataset, test_target, current_classes, batch_size, shuffle = False)\n",
        "\n",
        "        if task == 0 or non_incremental == True:\n",
        "            model.fc = nn.Linear(model.fc.in_features, len(current_classes)).to(device)\n",
        "        else:\n",
        "            # Expand the output layer for new classes\n",
        "            old_weight = model.fc.weight.data\n",
        "            old_bias = model.fc.bias.data\n",
        "            model.fc = nn.Linear(model.fc.in_features, len(current_classes)).to(device)\n",
        "            model.fc.weight.data[:len(old_weight)] = old_weight\n",
        "            model.fc.bias.data[:len(old_bias)] = old_bias\n",
        "\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "        scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "\n",
        "        print(f\"Starting Task {task+1} - Training on classes: {task_classes}\")\n",
        "        for epoch in range(num_epochs): # Adjust number of epochs\n",
        "            #train(model, train_loader, optimizer, criterion, device, epoch)\n",
        "\n",
        "            # If logging training (but not incremental)\n",
        "            train_wandb(model, train_loader, optimizer, criterion, device, epoch)\n",
        "\n",
        "            scheduler.step()\n",
        "            accuracy = evaluate(model, train_loader, device)\n",
        "            print(f\"Task {task+1}, Epoch {epoch+1}: Accuracy Train = {accuracy:.2f}%\")\n",
        "\n",
        "            val_metrics = {\"val/val_accuracy\": accuracy}\n",
        "            #wandb.log({**val_metrics})\n",
        "\n",
        "        accuracy = evaluate(model, test_loader, device)\n",
        "        accuracies.append(accuracy)\n",
        "        print(f\"Task {task+1}: Accuracy Test = {accuracy:.2f}%\")\n",
        "\n",
        "        incremental_metrics = {\"incremental_accuracy\": accuracy, \"task\": task}\n",
        "        wandb.log({**incremental_metrics})\n",
        "\n",
        "        # Log train and validation metrics to wandb\n",
        "\n",
        "    return accuracies\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAaNngOPM0Sq"
      },
      "source": [
        "## Pre-Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgJ8qN2tNDLh"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "root_dir = './data'  # Path to GTSRB dataset\n",
        "num_tasks = 5\n",
        "numclasses = len(np.unique(train_target))\n",
        "classes_per_task = numclasses // num_tasks #43/2 ~ 20\n",
        "batch_size = 64\n",
        "lr = 1e-3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "buffer_size = 200 # Adjust rehearsal set size\n",
        "alignment_strength = 0.1 # Adjust alignment strength\n",
        "num_epochs = 4\n",
        "\n",
        "#model = SimpleCNN(n_out = 1, n_in = 3).to(device)\n",
        "#model.apply(initialize_weights)\n",
        "\n",
        "# The name of the network (choose the on you want)\n",
        "tag = \"simpleCNN_GTSRB_pretrained\"\n",
        "netname = os.path.join(root_dir, 'network_{:s}.pth'.format(tag))\n",
        "\n",
        "#################################################\n",
        "## Pre-training\n",
        "####\n",
        "\n",
        "# Read the last learned network (if stored)\n",
        "if (os.path.exists(netname)):\n",
        "    print('Load pre-trained network')\n",
        "    model = SimpleCNN(n_in = 3, n_out=classes_per_task)\n",
        "    model.load_state_dict(torch.load(netname,weights_only=True))\n",
        "\n",
        "    #model = torch.load(netname, weights_only=True)\n",
        "    model = model.to(device)\n",
        "else:\n",
        "    print('Pretrain')\n",
        "    model = SimpleCNN(n_in = 3, n_out=1)\n",
        "    model.apply(initialize_weights)\n",
        "    model.to(device)\n",
        "\n",
        "    accu = incremental_learning(model, train_dataset, train_target, test_dataset, test_target,\n",
        "                        1, classes_per_task, batch_size, num_epochs, lr, device)\n",
        "\n",
        "    print(f\"!!!!! Pre-training on first task  = {accu[0]:.2f}%\")\n",
        "\n",
        "    # Save last learned model\n",
        "    #torch.save(model, netname)\n",
        "    torch.save(model.state_dict(), netname)\n",
        "\n",
        "## Copy model to have the same initialization\n",
        "copy_model = copy.deepcopy(model) # Copy model to start from the same initialization\n",
        "\n",
        "#### Learn with a single epoch in incremental (faster but less accurate)\n",
        "num_epochs = 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93lWmMn2Nnnw"
      },
      "source": [
        "## Fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tla2raJ7Nn_o"
      },
      "outputs": [],
      "source": [
        "#############################################\n",
        "## Fine tuning\n",
        "####\n",
        "# initialise a wandb run\n",
        "run = wandb.init(\n",
        "    project=\"GTSRB-CIL\",\n",
        "    name = \"Fine tuning\",\n",
        "    config={\n",
        "        \"epochs\": num_epochs,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"num_tasks\": num_tasks,\n",
        "        \"classes_per_task\": classes_per_task,\n",
        "        \"lr\": lr,\n",
        "        })\n",
        "\n",
        "# Simple Incremental Fine Tuning\n",
        "model = copy.deepcopy(copy_model)\n",
        "incremental_learning_wandb(model, train_dataset, train_target, test_dataset, test_target,\n",
        "                      num_tasks, classes_per_task, batch_size, num_epochs, lr, device)\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfV4H_gQlJOg"
      },
      "source": [
        "## Upper bound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xs4duQSA3LY8"
      },
      "outputs": [],
      "source": [
        "#################################################\n",
        "## Global upper bound (all data, all classes)\n",
        "####\n",
        "\n",
        "# One task + all classes computed using 5 epochs\n",
        "\n",
        "model = copy.deepcopy(copy_model)\n",
        "accu = incremental_learning(model, train_dataset, train_target, test_dataset, test_target,\n",
        "                      1, (numclasses // num_tasks) * num_tasks, batch_size, 5, lr, device)\n",
        "\n",
        "print(f\"!!!!! Upper bound of accuracy = {accu[0]:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQONaVy3MYRa"
      },
      "outputs": [],
      "source": [
        "########################################\n",
        "## Upper bound for each task (takes some time)\n",
        "####\n",
        "# initialise a wandb run\n",
        "run = wandb.init(\n",
        "    project=\"GTSRB-CIL\",\n",
        "    name = \"Upper bound\",\n",
        "    config={\n",
        "        \"epochs\": num_epochs,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"num_tasks\": num_tasks,\n",
        "        \"classes_per_task\": classes_per_task,\n",
        "        \"lr\": lr,\n",
        "        })\n",
        "\n",
        "# Non incremental data (learn all classes from all data for each task)\n",
        "model = copy.deepcopy(copy_model)\n",
        "incremental_learning_wandb(model, train_dataset, train_target, test_dataset, test_target,\n",
        "                      num_tasks, classes_per_task, batch_size, num_epochs, lr, device, non_incremental = True)\n",
        "\n",
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}